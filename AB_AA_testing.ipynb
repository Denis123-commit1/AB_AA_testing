{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Предположение: ожидаем увидеть конверсию, которая не позволит отклонить нулевую\n",
    "# гипотезу о случайной разнице конверсий двух веб-страниц\n",
    "# Нулевая гипотеза: разница конверсий (количества кликов в нашем случае) случайна\n",
    "# Альтернативная гипотеза: разница конверсий не случайна\n",
    "from ipywidgets import interact,IntSlider\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import warnings\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from io import StringIO\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import norm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from stattests.tests import *\n",
    "from stattests.generation import generate_data\n",
    "from stattests.utils import plot_cdf, plot_summary, codenames2titles,  frame_from_params, plot_from_params, save_gif_and_show\n",
    "from stattests.data import rpv, apply_all_tests\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from scipy.stats import arcsine\n",
    "from scipy.stats import beta\n",
    "\n",
    "\n",
    "\n",
    "colors = sns.color_palette(\"deep\")\n",
    "\n",
    "\n",
    "\n",
    "from typing import Tuple, Dict, Optional, Set, List\n",
    "\n",
    "import imageio\n",
    "from IPython.core.display import HTML\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Export_Data:\n",
    "\n",
    "#     filepath = 'C:/Users/Dell/Projects/mind_set/ab_testing/ab_data.csv'\n",
    "#     data = pd.read_csv(filepath)\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def export_data(filepath):\n",
    "        data = pd.read_csv(filepath)\n",
    "\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    data = export_data('C:/Users/Dell/Projects/mind_set/ab_testing/ab_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class N_required:\n",
    "\n",
    "    def __init__(self,  required_n_1, required_n):\n",
    "        self._required_n_1 = required_n_1\n",
    "        self.required_n = required_n\n",
    "\n",
    "\n",
    "\n",
    "    def required_n_1(effect_size_1, effect_size_2, power, alpha, ratio):\n",
    "        required_n = sms.NormalIndPower().solve_power(\n",
    "                effect_size=sms.proportion_effectsize(0.121, 0.123),\n",
    "                power=0.85,\n",
    "                alpha=0.01,\n",
    "                ratio=1\n",
    "            )  # Calculating sample size needed\n",
    "\n",
    "\n",
    "        required_n = ceil(required_n) # Округляем в сторону большего значения при необходимости\n",
    "\n",
    "\n",
    "        return required_n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Параметры можно настроить как нужно для задачи\n",
    "\n",
    "    required_n = required_n_1(0.121, 0.123, 0.85, 0.01, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Preprocessing_Data(Export_Data):\n",
    "\n",
    "\n",
    "    def __init__(self, data, data_crosstab, data_for_ab_test, preproc = None,\\\n",
    "                 conversion_rates_1 = None, preparation_for_Ztest = None):\n",
    "\n",
    "#         self._export_data = export_data # добавить в инит export data при необходимости\n",
    "        self.data = data\n",
    "        self.data_crosstab = data_crosstab\n",
    "        self.data_for_ab_test = data_for_ab_test\n",
    "        self._preproc = preproc\n",
    "        self._conversion_rates_1 = conversion_rates_1\n",
    "        self._preparation_for_Ztest =  preparation_for_Ztest\n",
    "\n",
    "\n",
    "    def preproc(data):\n",
    "\n",
    "        # создаем перекрестную таблицу для того, чтобы удостовериться, что в контрольной группе\n",
    "        # есть люди с старой и новой страницей\n",
    "\n",
    "        data_crosstab = pd.crosstab(data['group'], data['landing_page'])\n",
    "\n",
    "        # Делим данные на контрольную и тестовую группы\n",
    "        #\n",
    "        required_n = 100000\n",
    "        # Меняем random_state для нескольких проверок p-value\n",
    "        control_sample = data[data['group'] == 'control'].sample(n=required_n, random_state=22)\n",
    "        control_1_sample = data[data['group'] == 'control'].sample(n=required_n - 80000, random_state=18)\n",
    "        treatment_sample = data[data['group'] == 'treatment'].sample(n=required_n, random_state=22)\n",
    "\n",
    "        data_for_ab_test = pd.concat([control_sample, control_1_sample, treatment_sample], axis=0)\n",
    "        data_for_ab_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        # Делаем список из возврааемых переменных\n",
    "        return [data_crosstab, data_for_ab_test]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def conversion_rates_1(data):\n",
    "#         conversion_rates = data.groupby('group')['converted']\n",
    "\n",
    "#         std_p = lambda x: np.std(x, ddof=0)              # Std. deviation of the proportion\n",
    "#         se_p = lambda x: stats.sem(x, ddof=0)            # Std. error of the proportion (std / sqrt(n))\n",
    "\n",
    "#         conversion_rates = conversion_rates.agg([np.mean, std_p, se_p])\n",
    "#         conversion_rates.columns = ['conversion_rate', 'std_deviation', 'std_error']\n",
    "\n",
    "\n",
    "#         return conversion_rates.style.format('{:.3f}')\n",
    "\n",
    "\n",
    "        se_p = lambda x: stats.sem(x, ddof=0)\n",
    "        conversion_rates = data.groupby('group').agg({'converted': [np.mean, np.std, se_p]})\n",
    "        conversion_rates.columns = ['np_mean_conversion_rate', 'std_deviation', 'std_error']\n",
    "        return conversion_rates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Conducting_A_A_tests:\n",
    "\n",
    "    def __init__(self, data, conducting_Ztest_AA = None):\n",
    "        self.data = data\n",
    "        self._conducting_Ztest_AA = conducting_Ztest_AA\n",
    "\n",
    "\n",
    "\n",
    "    def conducting_Ztest_AA(data):\n",
    "\n",
    "\n",
    "        control_results = data[data['group'] == 'control']['converted']\n",
    "        control_1_results = data[data['group'] == 'control']['converted']\n",
    "\n",
    "        n_con = control_results.count()\n",
    "        n_con_1 = control_1_results.count()\n",
    "        successes = [control_results.sum(), control_1_results.sum()]\n",
    "        nobs = [n_con, n_con_1]\n",
    "\n",
    "        z_stat, pval = proportions_ztest(successes, nobs=nobs)\n",
    "        (lower_con, lower_con_1), (upper_con, upper_con_1) = proportion_confint(successes, nobs=nobs, alpha=0.05)\n",
    "\n",
    "        print(\"\"\"\n",
    "\n",
    "        Для АА-теста\n",
    "\n",
    "\n",
    "        \"\"\")\n",
    "        print(f'z statistic: {z_stat:.2f}')\n",
    "        print(f'p-value: {pval:.3f}')\n",
    "        print(f'ci 95% for control group: [{lower_con:.3f}, {upper_con:.3f}]')\n",
    "        print(f'ci 95% for control_1 group: [{lower_con_1:.3f}, {upper_con_1:.3f}]')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Conducting_A_B_tests:\n",
    "\n",
    "\n",
    "    def __init__(self, data, preparation_for_Ztest_AB = None,conducting_Ztest_AB = None):\n",
    "        self.data = data\n",
    "        self._conducting_Ztest_AB = conducting_Ztest_AB\n",
    "\n",
    "\n",
    "\n",
    "    def conducting_Ztest_AB(data):\n",
    "        control_results = data[data['group'] == 'control']['converted']\n",
    "        treatment_results = data[data['group'] == 'treatment']['converted']\n",
    "\n",
    "        n_con = control_results.count()\n",
    "        n_treat = treatment_results.count()\n",
    "        successes = [control_results.sum(), treatment_results.sum()]\n",
    "        nobs = [n_con, n_treat]\n",
    "\n",
    "        z_stat, pval = proportions_ztest(successes, nobs=nobs)\n",
    "        (lower_con, lower_treat), (upper_con, upper_treat) = proportion_confint(successes, nobs=nobs, alpha=0.05)\n",
    "\n",
    "        print(\"\"\"\n",
    "\n",
    "        Для АБ-теста\n",
    "\n",
    "        \"\"\")\n",
    "        print(f'z statistic: {z_stat:.2f}')\n",
    "        print(f'p-value: {pval:.3f}')\n",
    "        print(f'ci 95% for control group: [{lower_con:.3f}, {upper_con:.3f}]')\n",
    "        print(f'ci 95% for treatment group: [{lower_treat:.3f}, {upper_treat:.3f}]')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Modeling_AA_AB_tests():\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def plot_probab_contr_and_treat(size = 100):\n",
    "\n",
    "\n",
    "        # for control\n",
    "        np.random.seed()\n",
    "        n=1\n",
    "        p_contr=Preprocessing_Data.conversion_rates_1(Export_Data.data).iloc[0,0]\n",
    "\n",
    "        # for treatment\n",
    "        np.random.seed()\n",
    "        n=1\n",
    "        p_treat=Preprocessing_Data.conversion_rates_1(Export_Data.data).iloc[1,0]\n",
    "\n",
    "        # for control_1\n",
    "        np.random.seed()\n",
    "        n=1\n",
    "        p_contr_1=Preprocessing_Data.conversion_rates_1(Export_Data.data).iloc[0,0]\n",
    "\n",
    "\n",
    "\n",
    "    # let us repeat our experiment for 100000 times\n",
    "\n",
    "        x=np.random.binomial(n=n, p=p_contr, size=size)\n",
    "        probs_100= [sum(np.random.binomial(n,p_contr,size=size) == i)/size for i in range(n+1)]\n",
    "        plt.xticks(range(n+1))\n",
    "        plt.plot(list(range(n+1)), probs_100, color='blue', marker='o')\n",
    "        plt.xlabel('Number of Heads',fontsize=14)\n",
    "        plt.ylabel('Probability',fontsize=14)\n",
    "\n",
    "\n",
    "        x_1=np.random.binomial(n=n, p=p_treat, size=size)\n",
    "        probs_100= [sum(np.random.binomial(n,p_treat,size=size) == i)/size for i in range(n+1)]\n",
    "        plt.xticks(range(n+1))\n",
    "        plt.plot(list(range(n+1)), probs_100, color='red', marker='o')\n",
    "        plt.xlabel('Number of Heads',fontsize=14)\n",
    "        plt.ylabel('Probability',fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "        x_2=np.random.binomial(n=n, p=p_contr_1, size=size)\n",
    "        probs_100= [sum(np.random.binomial(n,p_contr_1,size=size) == i)/size for i in range(n+1)]\n",
    "        plt.xticks(range(n+1))\n",
    "        plt.plot(list(range(n+1)), probs_100, color='black', marker='o')\n",
    "        plt.xlabel('Number of Heads',fontsize=14)\n",
    "        plt.ylabel('Probability',fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "        n_con = size\n",
    "        n_treat = size\n",
    "        successes_contr_test = [x.sum(), x_1.sum()]\n",
    "        nobs_contr_test = [n_con, n_treat]\n",
    "\n",
    "        z_stat_contr_test, pval_contr_test = proportions_ztest(successes_contr_test, nobs=nobs_contr_test)\n",
    "        (lower_con_contr_test, lower_treat_contr_test),\\\n",
    "        (upper_con_contr_test, upper_treat_contr_test) = proportion_confint(successes_contr_test, nobs=nobs_contr_test, alpha=0.05)\n",
    "\n",
    "        print(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        Z-Test for Control-Treatment\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\")\n",
    "        print(f'z statistic_contr_test: {z_stat_contr_test:.2f}')\n",
    "        print(f'p-value_contr_test: {pval_contr_test:.3f}')\n",
    "        print(f'ci 95% for control group_contr_test: [{lower_con_contr_test:.3f}, {upper_con_contr_test:.3f}]')\n",
    "        print(f'ci 95% for treatment group_contr_test: [{lower_treat_contr_test:.3f}, {upper_treat_contr_test:.3f}]')\n",
    "\n",
    "\n",
    "        n_con = size\n",
    "        n_con_1 = size\n",
    "        successes_contr_contr_1 = [x.sum(), x_2.sum()]\n",
    "        nobs_contr_contr_1 = [n_con, n_con_1]\n",
    "\n",
    "        z_stat_contr_contr_1, pval_contr_contr_1 = proportions_ztest(successes_contr_contr_1, nobs=nobs_contr_contr_1)\n",
    "        (lower_con_contr_contr_1, lower_treat_contr_contr_1), \\\n",
    "        (upper_con_contr_contr_1, upper_treat_contr_contr_1) =  proportion_confint(successes_contr_contr_1, nobs=nobs_contr_contr_1, alpha=0.05)\n",
    "\n",
    "        print(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        Z-Test for Control - Control_1\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\")\n",
    "        print(f'z statistic_contr_contr_1: {z_stat_contr_contr_1:.2f}')\n",
    "        print(f'p-value_contr_contr_1: {pval_contr_contr_1:.3f}')\n",
    "        print(f'ci 95% for control group_contr_contr_1: [{lower_con_contr_contr_1:.3f}, {upper_con_contr_contr_1:.3f}]')\n",
    "        print(f'ci 95% for treatment group_contr_contr_1: [{lower_treat_contr_contr_1:.3f}, {upper_treat_contr_contr_1:.3f}]')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Visulation_different_methods:\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plotting_beta_distribution(a, b):\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')\n",
    "\n",
    "        x = np.linspace(beta.ppf(0.01, a, b),\n",
    "                        beta.ppf(0.99, a, b), 100)\n",
    "        ax.plot(x, beta.pdf(x, a, b),\n",
    "               'r-', lw=5, alpha=0.9, label='beta pdf')\n",
    "\n",
    "\n",
    "        rv = beta(a, b)\n",
    "        ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
    "\n",
    "\n",
    "        vals = beta.ppf([0.001, 0.5, 0.999], a, b)\n",
    "        np.allclose([0.001, 0.5, 0.999], beta.cdf(vals, a, b))\n",
    "\n",
    "\n",
    "        r = beta.rvs(a, b, size=1000)\n",
    "\n",
    "\n",
    "\n",
    "        ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
    "        ax.legend(loc='best', frameon=False)\n",
    "        print(\"\"\"\n",
    "        mean: {}\n",
    "        var: {}\n",
    "        skew: {}\n",
    "        kurt: {}\n",
    "        \"\"\".format(mean, var, skew, kurt))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def modeling_AA_AB_tests(success_rate, N, NN, beta, skew):\n",
    "\n",
    "\n",
    "    #     SELECTED PARAMETRES:\n",
    "\n",
    "#         success_rate = 0.12\n",
    "        uplift = 0.01\n",
    "#         N = 5000\n",
    "#         NN = 2000\n",
    "\n",
    "#         beta = 7\n",
    "#         skew = 1.44\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # VK-TEAM CODE\n",
    "\n",
    "\n",
    "        # PARAMS\n",
    "\n",
    "        skew_params = []\n",
    "        for s in np.linspace(0.1, 4, 20):\n",
    "            skew_params.append({'success_rate': success_rate, 'uplift': uplift,\\\n",
    "                                'beta': beta, 'skew': s, 'N': N, 'NN': NN})\n",
    "\n",
    "\n",
    "        beta_params = []\n",
    "        for b in np.logspace(0, 1, 20)[::-1]:\n",
    "            beta_params.append({'success_rate': success_rate, 'uplift': uplift, \\\n",
    "                                'beta': b, 'skew': skew, 'N': N, 'NN': NN})\n",
    "\n",
    "        sr_params = []\n",
    "        for sr in np.logspace(-3, -0.3, 20):\n",
    "            sr_params.append({'success_rate': sr, 'uplift': uplift, \\\n",
    "                              'beta': 500, 'skew': skew, 'N': N, 'NN': NN})\n",
    "\n",
    "\n",
    "        # DATA-GENERATION\n",
    "        # Параметр uplift регулируется до генерации данных\n",
    "#         for param in tqdm(beta_params + skew_params + sr_params):\n",
    "#             apply_all_tests('../data', **param)\n",
    "\n",
    "        # PICS\n",
    "\n",
    "        # Устанавливаем размер фигуры и её плотность на пиксель\n",
    "        figsize = (4, 3)\n",
    "        dpi = 200\n",
    "\n",
    "\n",
    "        # с помощью коэффициента Манна-Уитни проверяем гипотезу:\n",
    "        # Чем выше график ROC-AUC приближается к левой верхней части, тем лучше\n",
    "        # Для простоты можно смотреть на мощность теста: она максимальная - 1\n",
    "\n",
    "        #Вывод: Уверенно отвергается гипотеза о случайности разниц (!!!ДАННЫЕ СМЕНИТЬ)\n",
    "        fig, ax = plt.subplots(figsize=figsize, dpi=dpi) # пример генерации данных a/b и a/a теста\n",
    "        ab_data, aa_data = rpv('../data', 'mannwhitney_successes_count', N=N, NN=NN, uplift=uplift, beta=beta, success_rate=success_rate, skew=0.1)\n",
    "        plot_cdf(ab_data, 'MW', ax)\n",
    "        ax.plot(np.linspace(0, 1, 10000), np.linspace(0, 1, 10000), 'k', alpha=0.1)\n",
    "        ax.axvline(0.05, color='k', alpha=0.5) # 0,05 показывает как проходит вертикальная серая линия\n",
    "        ax.set_xlabel('FPR')\n",
    "        ax.set_ylabel('Sensitivity')\n",
    "\n",
    "\n",
    "        # Распределения просмотров у выборки А и В\n",
    "        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "        (views_0_ab, clicks_0_ab), _, gt_success_rates = generate_data(**skew_params[6])\n",
    "        sns.distplot(views_0_ab.flatten(), bins=range(0, 40), ax=ax, kde=False, norm_hist=True)\n",
    "        ax.set_title('Views distribution')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Распределение коэффиента конферсии на каждого пользователя\n",
    "        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "        sns.distplot(gt_success_rates.flatten(), ax=ax, kde=False, norm_hist=True)\n",
    "        ax.set_title('Ground truth user CTR distribution')\n",
    "\n",
    "\n",
    "        # Распределение кликов\n",
    "        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "        sns.distplot(clicks_0_ab.flatten(), bins=range(0, 10), ax=ax, kde=False, norm_hist=True)\n",
    "        ax.set_title('Clicks distribution')\n",
    "\n",
    "\n",
    "        # Просмотр результатов расчетов p-value и построения ROC-AUC кривой у АВ - ТЕСТА\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(figsize[0] * 2, figsize[1]), dpi=dpi)\n",
    "        ab_data, aa_data = rpv('../data', 'mannwhitney_successes_count', N=N, NN=NN, uplift=uplift, beta=beta, success_rate=success_rate, skew=0.1)\n",
    "        plot_cdf(ab_data, 'MW', ax2)\n",
    "\n",
    "        ax2.plot(np.linspace(0, 1, 10000), np.linspace(0, 1, 10000), 'k', alpha=0.1)\n",
    "        ax2.axvline(0.05, color='k', alpha=0.5)\n",
    "        ax2.set_title('Simulated p-value CDFs under H1')\n",
    "        ax2.set_xlabel('p-value')\n",
    "\n",
    "        ax1.set_title('Simulated p-value histogram under H1')\n",
    "        ax1.set_xlabel('p-value')\n",
    "        sns.distplot(ab_data.flatten(), ax=ax1, bins=100, kde=False, norm_hist=True)\n",
    "\n",
    "\n",
    "        # Встроенный АА - ТЕСТ и показ тех же самых результатов\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(figsize[0] * 2, figsize[1]), dpi=dpi)\n",
    "\n",
    "        ab_data, aa_data = rpv('../data', 'mannwhitney_successes_count', N=N, NN=NN, uplift=uplift, beta=beta, success_rate=success_rate, skew=0.1)\n",
    "        plot_cdf(aa_data, 'MW', ax2)\n",
    "\n",
    "        ax2.plot(np.linspace(0, 1, 10000), np.linspace(0, 1, 10000), 'k', alpha=0.1)\n",
    "        ax2.axvline(0.05, color='k', alpha=0.5)\n",
    "        ax2.set_title('Simulated p-value CDFs under H0 (FPR)')\n",
    "        ax2.set_xlabel('p-value')\n",
    "\n",
    "        ax1.set_title('Simulated p-value histogram under H0')\n",
    "        ax1.set_xlabel('p-value')\n",
    "        sns.distplot(aa_data.flatten(), ax=ax1, bins=10, kde=False, norm_hist=True)\n",
    "\n",
    "\n",
    "        # Интерпретация графиков\n",
    "        # Левый верхний график - Проведение АБ теста\n",
    "        # Правый верхний график - Проведение АА теста\n",
    "        # Левый нижный график - Мощность теста (место пересечения ROC-AUC с вертикальной\n",
    "        # линией в 0,05)\n",
    "        # Нижние правые графики - показ распределения в зависимости от разницы количества\n",
    "        # выборки и дисперсии\n",
    "\n",
    "        frames = [frame_from_params('../data', p, codenames=['ttest_successes_count', 'mannwhitney_successes_count','bootstrap', 'ttest_smoothed', 'buckets_ctrs', 'weighted_bootstrap', 'binomial_test']) for p in skew_params]\n",
    "\n",
    "\n",
    "        return save_gif_and_show('../gifs/preview.gif', frames)\n",
    "\n",
    "\n",
    "# Visulation_different_methods.modeling_AA_AB_tests\n",
    "\n",
    "    def vk_sandbox(success_rate, uplift, N, NN, beta, skew):\n",
    "\n",
    "        # Задаем параметры модели\n",
    "        ab_params = {'success_rate': success_rate, 'uplift': uplift, 'beta': beta, 'skew': skew, 'N': N, 'NN': NN}\n",
    "        aa_params = {'success_rate': success_rate, 'uplift': 0.0, 'beta': beta, 'skew': skew, 'N': N, 'NN': NN}\n",
    "\n",
    "        # Задаем графики распределений\n",
    "        (views_0_ab, clicks_0_ab), (views_1_ab, clicks_1_ab), gt_success_rates = generate_data(**ab_params)\n",
    "        (views_0_aa, clicks_0_aa), (views_1_aa, clicks_1_aa), _ = generate_data(**aa_params)\n",
    "\n",
    "        # Задаем параметры модели\n",
    "        linearized_0_ab, linearized_1_ab = linearization_of_clicks(clicks_0_ab, views_0_ab, clicks_1_ab, views_1_ab)\n",
    "        linearized_0_aa, linearized_1_aa = linearization_of_clicks(clicks_0_aa, views_0_aa, clicks_1_aa, views_1_aa)\n",
    "\n",
    "        corr_aware_w_0_ab, corr_aware_w_1_ab = intra_user_correlation_aware_weights(clicks_0_ab, views_0_ab, views_1_ab)\n",
    "        corr_aware_w_0_aa, corr_aware_w_1_aa = intra_user_correlation_aware_weights(clicks_0_aa, views_0_aa, views_1_aa)\n",
    "\n",
    "        smoothed_ctrs_0_ab, smoothed_ctrs_1_ab = get_smoothed_ctrs(clicks_0_ab, views_0_ab, clicks_1_ab, views_1_ab)\n",
    "        smoothed_ctrs_0_aa, smoothed_ctrs_1_aa = get_smoothed_ctrs(clicks_0_aa, views_0_aa, clicks_1_aa, views_1_aa)\n",
    "\n",
    "        global_ctr_0_ab = clicks_0_ab.sum(axis=1) / views_0_ab.sum(axis=1)\n",
    "        global_ctr_1_ab = clicks_1_ab.sum(axis=1) / views_1_ab.sum(axis=1)\n",
    "        global_ctr_0_aa = clicks_0_aa.sum(axis=1) / views_0_aa.sum(axis=1)\n",
    "        global_ctr_1_aa = clicks_1_aa.sum(axis=1) / views_1_aa.sum(axis=1)\n",
    "\n",
    "        results = {\n",
    "            'ttest_successes_count': (t_test(clicks_0_ab, clicks_1_ab), t_test(clicks_0_aa, clicks_1_aa), colors[0]),\n",
    "            'mannwhitney_successes_count': (mannwhitney(clicks_0_ab, clicks_1_ab), mannwhitney(clicks_0_aa, clicks_1_aa), colors[1]),\n",
    "            'delta': (delta_method_ctrs(clicks_0_ab, views_0_ab, clicks_1_ab, views_1_ab),\n",
    "                      delta_method_ctrs(clicks_0_aa, views_0_aa, clicks_1_aa, views_1_aa), colors[2]),\n",
    "            'bootstrap': (bootstrap(clicks_0_ab / views_0_ab, views_0_ab, clicks_1_ab / views_1_ab, views_1_ab),\n",
    "                          bootstrap(clicks_0_aa / views_0_aa, views_0_aa, clicks_1_aa / views_1_aa, views_1_aa), colors[3]),\n",
    "            'linearization': (t_test(linearized_0_ab, linearized_1_ab), t_test(linearized_0_aa, linearized_1_aa), colors[4]),\n",
    "            'buckets_ctrs': (bucketization(clicks_0_ab / views_0_ab, views_0_ab, clicks_1_ab / views_1_ab, views_1_ab),\n",
    "                             bucketization(clicks_0_aa / views_0_aa, views_0_aa, clicks_1_aa / views_1_aa, views_0_aa), colors[5]),\n",
    "            't_test_ctrs': (t_test(clicks_0_ab / views_0_ab, clicks_1_ab / views_1_ab),\n",
    "                            t_test(clicks_0_aa / views_0_aa, clicks_1_aa / views_1_aa), colors[6]),\n",
    "            'weighted_bootstrap': (bootstrap(clicks_0_ab / views_0_ab, corr_aware_w_0_ab, clicks_1_ab / views_1_ab, corr_aware_w_1_ab),\n",
    "                                   bootstrap(clicks_0_aa / views_0_aa, corr_aware_w_0_aa, clicks_1_aa / views_1_aa, corr_aware_w_1_aa), colors[7]),\n",
    "            'ttest_smoothed': (t_test(smoothed_ctrs_0_ab, smoothed_ctrs_1_ab), t_test(smoothed_ctrs_0_aa, smoothed_ctrs_1_aa), colors[8]),\n",
    "            'binomial_test': (binomial_test(global_ctr_0_ab, views_0_ab.sum(axis=1), global_ctr_1_ab, views_1_ab.sum(axis=1)),\n",
    "                          binomial_test(global_ctr_0_aa, views_0_aa.sum(axis=1), global_ctr_1_aa, views_1_aa.sum(axis=1)), colors[9])\n",
    "    }\n",
    "\n",
    "\n",
    "        # Создание списка для дальнейшего вырисовывания результатов\n",
    "        pretty_results = {}\n",
    "        for codename, p_values in results.items():\n",
    "            pretty_title = codename\n",
    "            if codename in codenames2titles:\n",
    "                pretty_title = codenames2titles[codename][0]\n",
    "            pretty_results[pretty_title] = p_values\n",
    "\n",
    "\n",
    "        # вырисовка результатов\n",
    "        plot_summary(pretty_results, views_0_aa, gt_success_rates)\n",
    "        return plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _main():\n",
    "\n",
    "\n",
    "    # Экспорт данных\n",
    "    #\n",
    "    export = Export_Data.data\n",
    "    print(export)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Необходимое количество юзеров для стат значимых результатов\n",
    "    #\n",
    "    n_required =  N_required.required_n\n",
    "    print(f\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Для того, чтобы результаты были статистически значимы и интерпретируемы,\n",
    "    нужно минимум {n_required} юзеров\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # вывдение перекрестной таблицы\n",
    "    #\n",
    "    preprocessing_data_data_crosstab = Preprocessing_Data.preproc(Export_Data.data)[0]\n",
    "    print(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\", preprocessing_data_data_crosstab)\n",
    "\n",
    "    # представление таблицы в виде двух выборок : контрольной и тестовой\n",
    "    #\n",
    "    preprocessing_data_data_for_ab_test = Preprocessing_Data.preproc(Export_Data.data)[1]\n",
    "    print(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "     \"\"\", preprocessing_data_data_for_ab_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # выведение таблицы со средним, стандартным отклонением\n",
    "    #\n",
    "\n",
    "    display(Preprocessing_Data.conversion_rates_1(Preprocessing_Data.preproc(Export_Data.data)[1]))\n",
    "\n",
    "\n",
    "\n",
    "    # Проведение Z-теста для АА теста\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(Conducting_A_A_tests.conducting_Ztest_AA(Preprocessing_Data.preproc(Export_Data.data)[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Проведение Z-теста для AB теста\n",
    "\n",
    "    print(Conducting_A_B_tests.conducting_Ztest_AB(Preprocessing_Data.preproc(Export_Data.data)[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Моделирование данных\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plot_probab_contr_and_treat = Modeling_AA_AB_tests.plot_probab_contr_and_treat\n",
    "    print(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "Моделирование данных с помощью биномиального распределения в зависимости от размера выборки:\n",
    "\n",
    "При размерах выборки больше 800 тысяч p-value уверенно меньше порогового значения a = 0,05\n",
    "При таких размерах выборки доверительный интервал выборочного среднего Контрольной выборки сдвинут\n",
    "правее чем интервал у Тестовой выборки. Вывод: при размерах выборки 800 тысяч уверенно отвергается\n",
    "гипотеза о случайности разниц средних: конверсия на контрольной выборки лучше чем на тестовой\n",
    "\n",
    "При меньших размерах выборки p-value неустойчивое и часто больше порогового значения a = 0,05\n",
    "\n",
    "\n",
    "Сравнение контрольных групп (разница будет из-за разницы, с которой работает функция random):\n",
    "На малых выборках(до 10 тысяч наблюдений) разница может быть значительной\n",
    "После нескольких сотен тысяч сгенерированных данных доверительный интервал сокращается и\n",
    "p-value показывает значение больше порогового значения a = 0,05,что говорит\n",
    "о случайной разнице между средними распределений\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\")\n",
    "    size_slider = IntSlider(min=100, max=1000000, step=100, value=100, description='$\\\\nu$')\n",
    "    interact(plot_probab_contr_and_treat,size=size_slider)\n",
    "\n",
    "\n",
    "\n",
    "    # Дальнейшая визуализация  с классом Visulation_different_methods\n",
    "\n",
    "    # Я хотел бы это показать на ответе доклада в зуме, так как это будет долго грузится\n",
    "\n",
    "    # Функция подбора параметров\n",
    "    plotting_beta_distribution = Visulation_different_methods.plotting_beta_distribution\n",
    "    plotting_beta_distribution(1, 7)\n",
    "\n",
    "\n",
    "    # Функция для визуализации gif\n",
    "    modeling_AA_AB_tests = Visulation_different_methods.modeling_AA_AB_tests\n",
    "    display(modeling_AA_AB_tests(0.12, 5000, 2000, 7, 1.44))\n",
    "    print(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "Судя по графикам видим:\n",
    "При конверсии на 1 процент ни один тест не показывает статистическую значимость.\n",
    "При увеличении количества выборки на АА тесте Z-тест дает большой FPR\n",
    "и видит разницу между двумя приблизительно одинаковыми выборками\n",
    "Вывод: при такой маленькой конверсии и количеством данных до миллиона нужно\n",
    "сделать вывод: разница между двумя выборками случайна\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    # Функция sandbox для для визуализации результатов всех тесто\n",
    "    vk_sandbox = Visulation_different_methods.vk_sandbox\n",
    "    vk_sandbox(0.12, 0.01, 5000, 2000, 7, 1.44)\n",
    "\n",
    "    print(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "Выводы по sandbox:\n",
    "При проведении АА теста только Z-test дает высокий FPR, остальные графики однородны\n",
    "При АВ тесте все графики показывают низкий FPR, кроме z-теста:\n",
    "при большой ассиметрии данных он дает сбои и отвергает нулевую гипотезу уверенно\n",
    "несмотря на результаты остальных тестов\n",
    "Вывод: при конверсии на 1% и выборке меньше 1 миллиона, гипотеза о статистической\n",
    "незначимости разницы принимается\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _main()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}